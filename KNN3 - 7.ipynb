{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wasi Khan\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\IBA\\ML\\Competition\\KNN3 - 7.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IBA/ML/Competition/KNN3%20-%207.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m test_predictions \u001b[39m=\u001b[39m knn_model\u001b[39m.\u001b[39mpredict_proba(test_data)[:, \u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IBA/ML/Competition/KNN3%20-%207.ipynb#W0sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Step 11: Create Submission File\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/IBA/ML/Competition/KNN3%20-%207.ipynb#W0sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m submission \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m\"\u001b[39m\u001b[39mRecordID\u001b[39m\u001b[39m\"\u001b[39m: test_data[\u001b[39m\"\u001b[39;49m\u001b[39mRecordID\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\u001b[39m.\u001b[39mvalues, \u001b[39m\"\u001b[39m\u001b[39mhospital_death\u001b[39m\u001b[39m\"\u001b[39m: test_predictions})\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/IBA/ML/Competition/KNN3%20-%207.ipynb#W0sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m submission\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39msubmission_knn.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Step 2: Load the Training and Test Data\n",
    "train_data = pd.read_csv(\"D:/IBA/ML/Competition/train.csv\")\n",
    "test_data = pd.read_csv(\"D:/IBA/ML/Competition/test.csv\")\n",
    "\n",
    "# Step 3: Handle Missing Values\n",
    "columns_with_missing = train_data.columns[train_data.isnull().mean() > 0.3].tolist()\n",
    "\n",
    "for col in columns_with_missing:\n",
    "    train_data[col] = train_data.groupby('hospital_id')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    test_data[col] = test_data.groupby('hospital_id')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Step 4: Split Data into Features and Target\n",
    "X = train_data.drop(columns=[\"hospital_death\"])\n",
    "y = train_data[\"hospital_death\"]\n",
    "\n",
    "# Step 5: One-Hot Encoding for Categorical Features\n",
    "categorical_features = [\"ethnicity\", \"gender\", \"icu_admit_source\", \"icu_stay_type\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\"]\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "X_encoded = encoder.fit_transform(X[categorical_features])\n",
    "feature_names = encoder.get_feature_names_out(input_features=categorical_features)\n",
    "X_categorical = pd.DataFrame(X_encoded, columns=feature_names)\n",
    "X.drop(columns=categorical_features, inplace=True)\n",
    "X = pd.concat([X, X_categorical], axis=1)\n",
    "\n",
    "# Step 6: Scale the Features\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "# Step 7: Impute Missing Values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Step 8: Train the K-Nearest Neighbors Model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=21)\n",
    "knn_model.fit(X, y)\n",
    "\n",
    "# Step 9: Preprocess the Test Data\n",
    "# Repeat the same preprocessing steps for the test data\n",
    "test_encoded = encoder.transform(test_data[categorical_features])\n",
    "test_categorical = pd.DataFrame(test_encoded, columns=feature_names)\n",
    "test_data.drop(columns=categorical_features, inplace=True)\n",
    "test_data = pd.concat([test_data, test_categorical], axis=1)\n",
    "test_data[numerical_features] = scaler.transform(test_data[numerical_features])\n",
    "test_data = imputer.transform(test_data)\n",
    "\n",
    "# Step 10: Make Predictions on Test Data\n",
    "test_predictions = knn_model.predict_proba(test_data)[:, 1]\n",
    "\n",
    "# Step 11: Create Submission File\n",
    "submission = pd.DataFrame({\"RecordID\": test_data[\"RecordID\"].astype(int).values, \"hospital_death\": test_predictions})\n",
    "submission.to_csv(\"submission_knn.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wasi Khan\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load training and test data\n",
    "train_data = pd.read_csv(\"D:/IBA/ML/Competition/train.csv\")  # Replace \"train.csv\" with your training file path\n",
    "test_data = pd.read_csv(\"D:/IBA/ML/Competition/test.csv\")    # Replace \"test.csv\" with your test file path\n",
    "\n",
    "# Data preprocessing\n",
    "# Separate categorical and numerical features\n",
    "categorical_features = [\"ethnicity\", \"gender\", \"icu_admit_source\", \"icu_stay_type\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\"]\n",
    "numerical_features = [col for col in train_data.columns if col not in [\"RecordID\", \"hospital_death\"] + categorical_features]\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_categorical = encoder.fit_transform(train_data[categorical_features])\n",
    "test_data_categorical = encoder.transform(test_data[categorical_features])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(train_data[numerical_features])\n",
    "test_data_numerical = scaler.transform(test_data[numerical_features])\n",
    "\n",
    "# Impute missing values in both training and test data\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_numerical_imputed = imputer.fit_transform(X_numerical)\n",
    "test_data_numerical_imputed = imputer.transform(test_data_numerical)\n",
    "\n",
    "# Create feature names for one-hot encoded columns\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine one-hot encoded categorical and standardized numerical features\n",
    "X = pd.DataFrame(X_categorical, columns=encoded_feature_names)\n",
    "X[numerical_features] = pd.DataFrame(X_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "test_data_processed = pd.DataFrame(test_data_categorical, columns=encoded_feature_names)\n",
    "test_data_processed[numerical_features] = pd.DataFrame(test_data_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "# Split the training data into features and target variable\n",
    "y = train_data[\"hospital_death\"]\n",
    "\n",
    "# Train K-Nearest Neighbor with 18 neighbors originally on the preprocessed data\n",
    "knn_model = KNeighborsClassifier(n_neighbors=200)\n",
    "knn_model.fit(X, y)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "knn_predictions = knn_model.predict_proba(test_data_processed)\n",
    "\n",
    "# Create submission DataFrames\n",
    "submission_knn = pd.DataFrame({\"RecordID\": test_data[\"RecordID\"], \"hospital_death\": knn_predictions[:, 1]})\n",
    "\n",
    "# Save submission file to CSV\n",
    "submission_knn.to_csv(\"submission_knn6 - 10.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wasi Khan\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer  # Import KNNImputer\n",
    "\n",
    "# Load training and test data\n",
    "train_data = pd.read_csv(\"D:/IBA/ML/Competition/train.csv\")  # Replace \"train.csv\" with your training file path\n",
    "test_data = pd.read_csv(\"D:/IBA/ML/Competition/test.csv\")    # Replace \"test.csv\" with your test file path\n",
    "\n",
    "# Data preprocessing\n",
    "# Separate categorical and numerical features\n",
    "categorical_features = [\"ethnicity\", \"gender\", \"icu_admit_source\", \"icu_stay_type\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\"]\n",
    "numerical_features = [col for col in train_data.columns if col not in [\"RecordID\", \"hospital_death\"] + categorical_features]\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_categorical = encoder.fit_transform(train_data[categorical_features])\n",
    "test_data_categorical = encoder.transform(test_data[categorical_features])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(train_data[numerical_features])\n",
    "test_data_numerical = scaler.transform(test_data[numerical_features])\n",
    "\n",
    "# KNN impute missing values in both training and test data\n",
    "knn_imputer = KNNImputer(n_neighbors=5)  # Set the number of neighbors\n",
    "X_numerical_imputed = knn_imputer.fit_transform(X_numerical)\n",
    "test_data_numerical_imputed = knn_imputer.transform(test_data_numerical)\n",
    "\n",
    "# Create feature names for one-hot encoded columns\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine one-hot encoded categorical and standardized numerical features\n",
    "X = pd.DataFrame(X_categorical, columns=encoded_feature_names)\n",
    "X[numerical_features] = pd.DataFrame(X_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "test_data_processed = pd.DataFrame(test_data_categorical, columns=encoded_feature_names)\n",
    "test_data_processed[numerical_features] = pd.DataFrame(test_data_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "# Split the training data into features and target variable\n",
    "y = train_data[\"hospital_death\"]\n",
    "\n",
    "# Train K-Nearest Neighbor with 200 neighbors on the preprocessed data\n",
    "knn_model = KNeighborsClassifier(n_neighbors=200)\n",
    "knn_model.fit(X, y)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "knn_predictions = knn_model.predict_proba(test_data_processed)\n",
    "\n",
    "# Create submission DataFrames\n",
    "submission_knn = pd.DataFrame({\"RecordID\": test_data[\"RecordID\"], \"hospital_death\": knn_predictions[:, 1]})\n",
    "\n",
    "# Save submission file to CSV\n",
    "submission_knn.to_csv(\"submission_knn7 - 11.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wasi Khan\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler  # Import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Load training and test data\n",
    "train_data = pd.read_csv(\"D:/IBA/ML/Competition/train.csv\")  # Replace with your training file path\n",
    "test_data = pd.read_csv(\"D:/IBA/ML/Competition/test.csv\")    # Replace with your test file path\n",
    "\n",
    "# Data preprocessing\n",
    "# Separate categorical and numerical features\n",
    "categorical_features = [\"ethnicity\", \"gender\", \"icu_admit_source\", \"icu_stay_type\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\"]\n",
    "numerical_features = [col for col in train_data.columns if col not in [\"RecordID\", \"hospital_death\"] + categorical_features]\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_categorical = encoder.fit_transform(train_data[categorical_features])\n",
    "test_data_categorical = encoder.transform(test_data[categorical_features])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(train_data[numerical_features])\n",
    "test_data_numerical = scaler.transform(test_data[numerical_features])\n",
    "\n",
    "# KNN impute missing values in both training and test data\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_numerical_imputed = knn_imputer.fit_transform(X_numerical)\n",
    "test_data_numerical_imputed = knn_imputer.transform(test_data_numerical)\n",
    "\n",
    "# Create feature names for one-hot encoded columns\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine one-hot encoded categorical and standardized numerical features\n",
    "X = pd.DataFrame(X_categorical, columns=encoded_feature_names)\n",
    "X[numerical_features] = pd.DataFrame(X_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "test_data_processed = pd.DataFrame(test_data_categorical, columns=encoded_feature_names)\n",
    "test_data_processed[numerical_features] = pd.DataFrame(test_data_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "# Split the training data into features and target variable\n",
    "y = train_data[\"hospital_death\"]\n",
    "\n",
    "# Apply Min-Max scaling to ensure non-negative values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "test_data_scaled = min_max_scaler.transform(test_data_processed)\n",
    "\n",
    "# Feature selection using SelectKBest with chi-squared statistics\n",
    "# Select the top 15 features based on statistical significance\n",
    "selector = SelectKBest(chi2, k=15)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "test_data_selected = selector.transform(test_data_scaled)\n",
    "\n",
    "# Train K-Nearest Neighbor with 200 neighbors on the selected features\n",
    "knn_model = KNeighborsClassifier(n_neighbors=200)\n",
    "knn_model.fit(X_selected, y)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "knn_predictions = knn_model.predict_proba(test_data_selected)\n",
    "\n",
    "# Create submission DataFrames\n",
    "submission_knn = pd.DataFrame({\"RecordID\": test_data[\"RecordID\"], \"hospital_death\": knn_predictions[:, 1]})\n",
    "\n",
    "# Save submission file to CSV\n",
    "submission_knn.to_csv(\"submission_knn8 - 12.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wasi Khan\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load training and test data\n",
    "train_data = pd.read_csv(\"D:/IBA/ML/Competition/train.csv\")\n",
    "test_data = pd.read_csv(\"D:/IBA/ML/Competition/test.csv\")\n",
    "\n",
    "# Data preprocessing\n",
    "categorical_features = [\"ethnicity\", \"gender\", \"icu_admit_source\", \"icu_stay_type\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\"]\n",
    "numerical_features = [col for col in train_data.columns if col not in [\"RecordID\", \"hospital_death\"] + categorical_features]\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_categorical = encoder.fit_transform(train_data[categorical_features])\n",
    "test_data_categorical = encoder.transform(test_data[categorical_features])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(train_data[numerical_features])\n",
    "test_data_numerical = scaler.transform(test_data[numerical_features])\n",
    "\n",
    "# KNN impute missing values in both training and test data\n",
    "knn_imputer = KNNImputer(n_neighbors=7)\n",
    "X_numerical_imputed = knn_imputer.fit_transform(X_numerical)\n",
    "test_data_numerical_imputed = knn_imputer.transform(test_data_numerical)\n",
    "\n",
    "# Combine one-hot encoded categorical and standardized numerical features\n",
    "X = pd.DataFrame(X_categorical, columns=encoder.get_feature_names_out(categorical_features))\n",
    "X[numerical_features] = pd.DataFrame(X_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "test_data_processed = pd.DataFrame(test_data_categorical, columns=encoder.get_feature_names_out(categorical_features))\n",
    "test_data_processed[numerical_features] = pd.DataFrame(test_data_numerical_imputed, columns=numerical_features)\n",
    "\n",
    "# Split the training data into features and target variable\n",
    "y = train_data[\"hospital_death\"]\n",
    "\n",
    "# Use Decision Tree to obtain feature importance scores\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = dt_model.feature_importances_\n",
    "\n",
    "# Sort features by importance and select the top k features\n",
    "k = 15  # Number of top features to select\n",
    "selected_feature_indices = feature_importances.argsort()[-k:][::-1]\n",
    "\n",
    "# Train KNN model on the selected features\n",
    "knn_model = KNeighborsClassifier(n_neighbors=200)\n",
    "knn_model.fit(X.iloc[:, selected_feature_indices], y)\n",
    "\n",
    "# Make predictions using the trained KNN model\n",
    "knn_predictions = knn_model.predict_proba(test_data_processed.iloc[:, selected_feature_indices])\n",
    "\n",
    "# Create submission DataFrames\n",
    "submission_knn = pd.DataFrame({\"RecordID\": test_data[\"RecordID\"], \"hospital_death\": knn_predictions[:, 1]})\n",
    "\n",
    "# Save submission file to CSV\n",
    "submission_knn.to_csv(\"submission_knn9 - 15(200 neighbours).csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
